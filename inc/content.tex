\makeatletter
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section\else
  \csname the#1\endcsname\quad
  \fi}
\makeatother
\section{Introduction}

Data retrieval in databases is typically done using SQL (Structured Query Language). Text-to-SQL machine learning models are a recent development in state-of-the-art research. The technique is an attractive alternative for many natural language problems, including complex queries and extraction tasks. The text is converted into a SQL query that can be executed on the database. This technique can save time and effort for both developers and end-users by enabling them to interact with databases through natural language queries. With the help of machine learning and knowledge-based resources, text language to SQL conversion is facilitated.

Semantic parsing is a natural language processing that extracts the meaning from text. Text-to-SQL, a type of Semantic Parsing, is a task that converts natural language problems into SQL query statements. 
This is achieved using machine learning and natural language processing algorithms, and this research is conducted to study different solutions and practices which has been taken by researchers to tackle this problem.

Text-to-SQL allows the elaboration of structured data with information about the natural language text in several domains, such as healthcare, customer service, and search engines. It can be used by data analysts, data scientists, software engineers, and end users who want to explore and analyze their data without learning SQL.
It can be used in a variety of ways:

1) Data analysts can use it to generate SQL queries for specific business questions, such as "What are the top ten products sold this month?"

2) Data scientists can use it to generate SQL queries for machine learning experiments, such as "How does the price of these products affect their sales?"

3) Businesses can use this technique to automate data extraction and improve efficiency.

4) End-users who want to explore and analyze their data without learning SQL can use it by clicking on a button on any table or chart in a user interface.

Although these models may not solve this problem entirely and perfectly, humans can still struggle with the task. For example, people involved in database migration projects often have to work on schema that they have never seen before.

This research study will review some of the most commonly used NLP technologies relevant to converting text language into Structured Query Language (SQL), and representative models and datasets in the recent solutions for this challenge and their technical implementation.

\newpage

\section{Motivation and State of the Art}

Representative datasets for Text-to-SQL include the WikiSQL\cite{zhong_seq2sql_2017} dataset and the SPIDER\cite{yu_spider_2019} dataset, which contains more complex SQL queries. 
% We will also take a shallow look at older datasets and why they are not in use anymore in Text-to-SQL studies.
The former case consists of Single Table - Multiple Question and the latter case Multiple Table - Multiple Question. The top models available for Text-To-SQL will be studied, and the implementation of a couple of currently best methods will be reviewed and discussed in this study. 

In this thesis, we will review the Text-to-SQL Challenges and datasets and structure of existing datasets and difference between them. Datasets to be covered are:
\begin{itemize}
\item ATIS
\item GeoQuery
\item IMDb
\item Advising
\item WikiSQL
\item Spider
\end{itemize}
% Furthermore, Text-to-SQL models and techniques represented in the SPIDER challenge will be studied and evaluated. 
The research section will assess the best state-of-the-art research in this field, the T5\cite{raffel_exploring_2020} Transformer with PICARD\cite{scholak_picard_2021} (2021), RAT-SQL\cite{wang_rat-sql_2021} (2019), BRIDGE v1 and v2\cite{lin_bridging_2020} with BERT, HydraNet\cite{lyu_hybrid_2020} (2020), some novel methods in recent years, and some older models like Seq2SQL\cite{zhong_seq2sql_2017} (2017) and SQLova\cite{hwang_comprehensive_2019} (2019).

\begin{itemize}
\item Seq2SQL and SQLNet will perform encoding using LSTM/Bi-LSTM and decode using classification and Pointer Network. 
\item SQLova and HydraNet encode natural language queries through a language model and decode them through the Natural Language-to-SQL layer, converting them to SQL grammars. Going a little further, SQLova was the first to use a language model as an encoder, encode questions and columns, and then predict queries. 
\item HydraNet uses BERT's token to rank columns one by one to fill in the slots of the SQL Query statement. Brief description of the BERT will be given in our final report.
\item Since the SPIDER dataset used for RAT-SQL and BRIDGE is a Multi-Table, it is necessary to understand the Schema's relationship to the question and its internal relationships, for which Schema Linking and Encoding are applied, and decoders such as SemQL are utilized. 
\item RAT SQL reflects the schema information into the encoding and decoding with SemQL, including the relationships extracted from the question-scheme contextualized graph in Self-Attention. 
\item BRIDGE achieves Schema Linking by including the Schema's Table and Column Name and Value in the Encoding and utilizes the Pointer Generator Network as the Decoder.
\newpage
\item PICARD proposes a new method for simple and effective constrained decoding with large pre-trained language models. 
On both the SPIDER cross-domain and cross-database Text-to-SQL dataset and the CoSQL SQL-grounded dialog state tracking dataset, we find that the PICARD decoding method not only significantly improves the performance of fine-tuned unmodified T5 models but it also lifts a T5-3B model to state-of-the-art results on the established exact-match and execution accuracy metrics.
\end{itemize}

After reviewing research papers of these models, we will study implementation steps of these models. And at last, we will use our private dataset to evaluate the accuracy of each models for our data and if they are usable and reliable enough for our usage. 

Most of these studies have excellent documentation regarding their implantation. Execution of these studies will be documented and published on Github. Nonetheless, In case of old and impractical implementation instructions, we will skip the implementation and continue with the top models available.

\subsection{SQLNet}


% ? The state-of-the-art should show the reader a broad overview of techniques available and how they are related to one another in a hierarchical way like a mindmap. Describe the methods briefly and explain the weaknesses and strengths of the methods and how you want to use them to solve your problem.


\section{Outline}

\begin{enumerate}

\item Introduction
\subitem Description and Motivation
\subitem Applications
\subitem Basic Research
\item Related Works and Background
\subitem Theoretical background and the review of world literature
\subitem Deep Learning and other approaches
\item Dataset, Implementation, and Results
\subitem Datasets and Challenges
\subitem Study Different Researches in Text-to-SQL SPIDER Challenge
\subsubitem Seq2SQL
\subsubitem RATSQL
\subsubitem T5 - PICARD
\subsubitem ...
\subitem Implementation Details
\item Summary and Future Work
\subitem Implement and test on private dataset
\subitem Discussion of the Results
\subitem Effect on other researches, like Text-to-SPARQL
\subitem Conclusions
\item Bibliography

\end{enumerate}

% \newpage

\section{Timeline}

The timeline for the project will be broken down into four phases: literature review, establishing the theoretical framework and research implementation, using some datasets on a state-of-the-art model, and finally, documenting our study.

Phase One: Literature Review
The first phase will be a review of any literature considerations. It includes examining how other researchers have approached the topic and a discussion of challenges around this research problem. Many considerations need to be addressed in this phase, such as the goal, the data, and how researchers find solutions for such a task.

Phase Two: Theoretical Framework and Research implementation
The second phase is where A theoretical framework of a model will be established and implemented. In order to do this, a variety of different approaches will be explored, which are possible to implement with our hardware and legal license limitations.

Phase Three: Using private data on our model
Here, the previously collected dataset will be used to test our model and validate the accuracy of the final result.

Phase Four: Documenting our study.
It will be planned to finish a draft regarding our study and our outcome in a month, and getting prepared for thesis defense.

The following timeline is a rough draft of what I would like to do for this project. It is not a complete timeline, and it is subject to change.


\begin{tabular}{||l|c|p{9.5cm}|}
    \cline{1-3}
    
    Week Number & Month & Task Detail \\
    \hline \hline
    22-23 & October 2022 & Lorem ipsum dolor sit amet, consectetuer adipiscing elit. \\
    \hline
    12 & November 2022 &  Ut purus elit, vestibulum ut, placerat ac, adipiscing vitae, felis. \\
    \hline
    12 & December 2022 & Curabitur dictum gravidamauris. \\
    \hline
    12 & January 2023 & Curabitur dictum gravidamauris. \\ [.3cm]
   \hline
    12 & February 2023 & Master's Thesis Defense \\ [.3cm]
    \cline{1-3}
\end{tabular}

\newpage