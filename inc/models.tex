\subsection{Models}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.99\textwidth]{pics/Timeline.png}
    \caption{An overview of the deep learning process for Text-to-SQL.}
    \label{fig:timeline}
\end{figure}

An efficient text-to-SQL solution requires state-of-the-art natural language processing techniques.
As a result of the neural network's ability to handle only numerical inputs and not raw text, word embedding has been used to represent numerical words.

Aside from that, in the past few years, language models have become increasingly popular as a solution for increasing performance in natural language processing tasks.

Assuming that words have numerical representations that differ from those of other words, word embeddings aim to map each word to a multidimensional vector, incorporating valuable information about the word. In addition to the brute-force creation of one-hot embeddings, researchers have developed highly efficient methods for creating representations that convey a word's meaning and relationships with other words. In most, if not all, Text-to-SQL systems, word embedding techniques such as Word2Vec\cite{DBLP:journals/corr/Rong14}, GloVe, and WordPiece embeddings\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16} are used.

Recently Language models have been shown to excel at NL tasks as a new type of pre-trained neural network. It is important to note that language models are not a replacement for word embeddings since they are neural networks and need a way to transform words into vectors.

Depending on the specific problem they want to solve, researchers can adapt the pre-trained model's inputs and outputs and train it for an additional number of epochs on their dataset. Thus, we can achieve state-of-the-art performance without complex architectures \cite{DBLP:journals/corr/abs-1810-04805}. Recent neural network architectures, like the Transformer\cite{DBLP:journals/corr/VaswaniSPUJGKP17}, have been used to achieve such performance by these models, which excel at handling NL and sequences of NL that are characterized by connections between words. Several language models have been used to handle the text-to-SQL task, including BERT \cite{DBLP:journals/corr/abs-1810-04805} and MT-DNN \cite{DBLP:journals/corr/abs-1901-11504}, while new models pre-trained specifically for structured data tasks are emerging, such as TaBERT\cite{DBLP:journals/corr/abs-2005-08314} and GraPPa \cite{DBLP:journals/corr/abs-2009-13845}.

% Furthermore, Text-to-SQL models and techniques represented in the SPIDER challenge will be studied and evaluated. 
The research section will assess the best state-of-the-art research in this field, starting from Seq2SQL\cite{zhong_seq2sql_2017} study in 2017 with the hype in WikiSQL challange and we will continue with SQLova\cite{hwang_comprehensive_2019} SQLNet\cite{xu_sqlnet_2017} and indtroduction of transformers and BERT with focus on RAT-SQL\cite{wang_rat-sql_2021} (2019), BRIDGE\cite{lin_bridging_2020} with BERT, HydraNet\cite{lyu_hybrid_2020} (2020), and the most recent solution with Google T5\cite{raffel_exploring_2020}, PICARD\cite{scholak_picard_2021} in SPIDER (2021).

% ////////////////////////////////////////////////////////////////////////////////

After reviewing the research papers on these models, we will study the implementation steps of these models. Moreover, evaluation methods and approaches to compare these models in accuracy for different datasets and if they are usable and reliable enough for our usage.

Most of these studies have excellent documentation regarding their implantation. Execution of these studies will be documented and published on Github. Nonetheless, In case of old and impractical implementation instructions, we will skip the implementation and continue with the top models available.

\input{inc/models/Seq2sql}
\input{inc/models/SQLNet}
\input{inc/models/SyntaxSQLNet}
\input{inc/models/GrammarSQL}
\input{inc/models/IRNet}
\input{inc/models/EditSQL}
\input{inc/models/RAT-SQL}
\input{inc/models/BRIDGE}
\input{inc/models/Picard}

% \begin{itemize}
%   \item Seq2SQL and SQLNet will perform encoding using LSTM/Bi-LSTM and decode using classification and Pointer Network.
%   \item SQLova and HydraNet encode natural language queries through a language model and decode them through the Natural Language-to-SQL layer, converting them to SQL grammars. Going a little further, SQLova was the first to use a language model as an encoder, encode questions and columns, and then predict queries.
%   \item HydraNet uses BERT's token to rank columns one by one to fill in the slots of the SQL Query statement. Brief description of the BERT will be given in our final report.
%   \item Since the SPIDER dataset used for RAT-SQL and BRIDGE is a Multi-Table, it is necessary to understand the Schema's relationship to the question and its internal relationships, for which Schema Linking and Encoding are applied, and decoders such as SemQL are utilized.
%   \item RAT-SQL reflects the schema information into the encoding and decoding with SemQL, including the relationships extracted from the question-scheme contextualized graph in Self-Attention.
%   \item BRIDGE achieves Schema Linking by including the Schema's Table and Column Name and Value in the Encoding and utilizes the Pointer Generator Network as the Decoder.
%   \item PICARD proposes a new method for simple and effective constrained decoding with large pre-trained language models.
%         On both the SPIDER cross-domain and cross-database Text-to-SQL dataset and the CoSQL SQL-grounded dialog state tracking dataset, we find that the PICARD decoding method not only significantly improves the performance of fine-tuned unmodified T5 models but it also lifts a T5-3B model to state-of-the-art results on the established exact-match and execution accuracy metrics.
% \end{itemize}